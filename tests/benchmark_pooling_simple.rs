use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use pgsqlite::session::DbHandler;

/// Simple concurrent benchmark to test connection pooling vs single connection

#[tokio::test]
async fn test_concurrent_reads_baseline() {
    println!("ðŸ§ª Testing concurrent reads (baseline - single connection)");
    
    let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos();
    let db_path = format!("/tmp/pooling_simple_baseline_{timestamp}.db");
    let db_handler = Arc::new(DbHandler::new(&db_path).unwrap());
    setup_test_data(&db_handler).await;
    
    let start = Instant::now();
    let mut tasks = Vec::new();
    
    // Run 4 concurrent read tasks for 2 seconds
    for _i in 0..4 {
        let db = db_handler.clone();
        let task = tokio::spawn(async move {
            let mut count = 0;
            let end_time = Instant::now() + Duration::from_secs(2);
            
            while Instant::now() < end_time {
                match db.query("SELECT COUNT(*) FROM test_data").await {
                    Ok(_) => count += 1,
                    Err(e) => eprintln!("Query error: {e}"),
                }
                tokio::time::sleep(Duration::from_micros(100)).await;
            }
            count
        });
        tasks.push(task);
    }
    
    let mut total_queries = 0;
    for task in tasks {
        total_queries += task.await.unwrap();
    }
    
    let duration = start.elapsed();
    let qps = total_queries as f64 / duration.as_secs_f64();
    
    println!("ðŸ“Š Baseline Results:");
    println!("  Total queries: {total_queries}");
    println!("  Duration: {:.2}s", duration.as_secs_f64());
    println!("  QPS: {qps:.0}");
    
    assert!(total_queries > 400, "Should execute at least 400 queries");
    
    // Cleanup
    let _ = std::fs::remove_file(&db_path);
    let _ = std::fs::remove_file(format!("{db_path}-wal"));
    let _ = std::fs::remove_file(format!("{db_path}-shm"));
}

#[tokio::test]
async fn test_write_contention() {
    println!("ðŸ§ª Testing write contention (SQLite single writer limitation)");
    
    let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos();
    let db_path = format!("/tmp/pooling_simple_write_{timestamp}.db");
    let db_handler = Arc::new(DbHandler::new(&db_path).unwrap());
    setup_test_data(&db_handler).await;
    
    let start = Instant::now();
    let mut tasks = Vec::new();
    
    // Run 4 concurrent write tasks for 1 second
    for task_id in 0..4 {
        let db = db_handler.clone();
        let task = tokio::spawn(async move {
            let mut success_count = 0;
            let mut error_count = 0;
            let end_time = Instant::now() + Duration::from_secs(1);
            
            while Instant::now() < end_time {
                let value = rand::random::<i32>() % 1000;
                match db.execute(&format!(
                    "UPDATE test_data SET value = {} WHERE id = {}",
                    value,
                    task_id + 1
                )).await {
                    Ok(_) => success_count += 1,
                    Err(_) => error_count += 1,
                }
                tokio::time::sleep(Duration::from_millis(1)).await;
            }
            (success_count, error_count)
        });
        tasks.push(task);
    }
    
    let mut total_success = 0;
    let mut total_errors = 0;
    for task in tasks {
        let (success, errors) = task.await.unwrap();
        total_success += success;
        total_errors += errors;
    }
    
    let duration = start.elapsed();
    let updates_per_second = total_success as f64 / duration.as_secs_f64();
    
    println!("ðŸ“Š Write Contention Results:");
    println!("  Successful updates: {total_success}");
    println!("  Failed updates: {total_errors}");
    println!("  Duration: {:.2}s", duration.as_secs_f64());
    println!("  Updates/sec: {updates_per_second:.0}");
    
    // Expect some failures due to write lock contention
    println!("  Note: Failures are expected due to SQLite's single writer limitation");
    
    // Cleanup
    let _ = std::fs::remove_file(&db_path);
    let _ = std::fs::remove_file(format!("{db_path}-wal"));
    let _ = std::fs::remove_file(format!("{db_path}-shm"));
}

#[tokio::test]
async fn test_read_write_mix() {
    println!("ðŸ§ª Testing mixed read/write workload");
    
    let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_nanos();
    let db_path = format!("/tmp/pooling_simple_mix_{timestamp}.db");
    let db_handler = Arc::new(DbHandler::new(&db_path).unwrap());
    setup_test_data(&db_handler).await;
    
    let start = Instant::now();
    let mut tasks = Vec::new();
    
    // 3 readers, 1 writer
    for i in 0..4 {
        let db = db_handler.clone();
        let is_writer = i == 3;
        
        let task = tokio::spawn(async move {
            let mut count = 0;
            let end_time = Instant::now() + Duration::from_secs(2);
            
            while Instant::now() < end_time {
                if is_writer {
                    let value = rand::random::<i32>() % 1000;
                    match db.execute(&format!(
                        "UPDATE test_data SET value = {value} WHERE id = 1"
                    )).await {
                        Ok(_) => count += 1,
                        Err(_) => {} // Ignore write errors
                    }
                    tokio::time::sleep(Duration::from_millis(10)).await;
                } else {
                    match db.query("SELECT AVG(value) FROM test_data").await {
                        Ok(_) => count += 1,
                        Err(e) => eprintln!("Read error: {e}"),
                    }
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }
            }
            (count, is_writer)
        });
        tasks.push(task);
    }
    
    let mut total_reads = 0;
    let mut total_writes = 0;
    for task in tasks {
        let (count, is_writer) = task.await.unwrap();
        if is_writer {
            total_writes = count;
        } else {
            total_reads += count;
        }
    }
    
    let duration = start.elapsed();
    let reads_per_second = total_reads as f64 / duration.as_secs_f64();
    let writes_per_second = total_writes as f64 / duration.as_secs_f64();
    
    println!("ðŸ“Š Mixed Workload Results:");
    println!("  Total reads: {total_reads}");
    println!("  Total writes: {total_writes}");
    println!("  Duration: {:.2}s", duration.as_secs_f64());
    println!("  Reads/sec: {reads_per_second:.0}");
    println!("  Writes/sec: {writes_per_second:.0}");
    
    assert!(total_reads > 200, "Should execute at least 200 reads");
    assert!(total_writes > 20, "Should execute at least 20 writes");
    
    // Cleanup
    let _ = std::fs::remove_file(&db_path);
    let _ = std::fs::remove_file(format!("{db_path}-wal"));
    let _ = std::fs::remove_file(format!("{db_path}-shm"));
}

async fn setup_test_data(db_handler: &DbHandler) {
    db_handler.execute("
        CREATE TABLE IF NOT EXISTS test_data (
            id INTEGER PRIMARY KEY,
            value INTEGER NOT NULL,
            description TEXT
        )
    ").await.unwrap();
    
    for i in 1..=10 {
        db_handler.execute(&format!(
            "INSERT INTO test_data (id, value, description) VALUES ({}, {}, 'test_data_{}')",
            i, i * 100, i
        )).await.unwrap();
    }
}