# pgsqlite Project Context

## Overview
pgsqlite is a PostgreSQL protocol adapter for SQLite databases. It allows PostgreSQL clients to connect to and query SQLite databases using the PostgreSQL wire protocol.

## Project Structure
- `src/` - Main source code directory
  - `lib.rs` - Main library entry point
  - `protocol/` - PostgreSQL wire protocol implementation
  - `session/` - Session state management
  - `query/` - Query execution handlers
- `tests/` - Test files
- `Cargo.toml` - Rust project configuration
- `TODO.md` - Comprehensive task list for future development

## Build Commands
- `cargo build` - Build the project
- `cargo test` - Run tests
- `cargo run` - Run the project

## Development Workflow
- After implementing any feature, always run the full test suite with `cargo test` to ensure nothing is broken
- **ALWAYS update TODO.md when completing work or discovering new tasks**:
  - Mark completed tasks with `[x]`
  - Add new discovered tasks or subtasks
  - Document partial progress with detailed notes
  - Update task descriptions if implementation reveals complexity
- Check TODO.md for prioritized tasks when planning development work
- Use TODO.md as the authoritative source for tracking all future work
- **NEVER commit code before ensuring ALL of the following pass**:
  - `cargo check` - No compilation errors or warnings
  - `cargo build` - Successfully builds the project
  - `cargo test` - All tests pass
  - If any of these fail, fix the issues before committing

## Code Style
- Follow Rust conventions
- Use existing imports and patterns
- Avoid adding comments unless necessary
- Keep code concise and idiomatic

## Recent Work (Condensed History)
- Implemented comprehensive PostgreSQL type support (40+ types including ranges, network types, binary types)
- Built custom DECIMAL type system with automatic query rewriting for proper numeric handling
- Developed multi-phase SELECT query optimization reducing overhead from ~200x to ~14x for cached queries:
  - Phase 1: Query plan cache with LRU eviction
  - Phase 2: Enhanced fast path for simple WHERE clauses and parameters
  - Phase 3: Prepared statement pooling with metadata caching
  - Phase 4: Schema cache with bulk preloading and bloom filters
  - Phase 5: Execution cache with query fingerprinting and optimized type conversion
  - Phase 6: Binary protocol support and result caching
- Implemented zero-copy protocol architecture:
  - Phase 1-5: Memory-mapped values, direct socket writing, buffer pooling
  - Achieved 67% improvement in cached SELECT queries (26x → 8.5x overhead)
- Optimized INSERT operations:
  - Fast path detection and execution for non-decimal tables
  - Statement pool provides near-native performance (1.0x overhead)
  - Protocol overhead remains significant (~168x) due to PostgreSQL wire protocol

## Known Issues
- **BIT type casts**: Prepared statements with multiple columns containing BIT type casts may return empty strings instead of the expected bit values. This is a limitation in the current execution cache implementation.
- **Array types**: Array handling is not yet implemented
- **Extended protocol parameter type inference**: Some parameter types may require explicit casts

## Important Design Decisions
- **Type Inference**: NEVER use column names to infer types. Types should be determined from:
  - Explicit PostgreSQL type declarations in CREATE TABLE statements
  - SQLite schema information via PRAGMA table_info
  - Explicit type casts in queries (e.g., $1::int4)
  - Value-based inference only when schema information is unavailable

- **Decimal Query Rewriting**: 
  - Only NUMERIC types (stored as DECIMAL in SQLite) require decimal_from_text wrapping for aggregates
  - FLOAT types (REAL, DOUBLE PRECISION, FLOAT4, FLOAT8) should NOT be wrapped as they're already decimal-compatible
  - Correlated subqueries must inherit outer context to recognize outer table columns
  - Context merging is essential for proper type resolution in nested queries

## Quality Standards
To avoid idiot behavior:
- Write tests that actually verify functionality, not tests that are designed to pass easily
- Only mark tasks as complete when they are actually finished and working
- Test edge cases and error conditions, not just happy paths
- Verify implementations work end-to-end, not just in isolation
- Don't claim something works without actually testing it

## Database Handler Architecture (2025-06-30)

### Background
The initial implementation used a channel-based approach with a dedicated thread for SQLite operations. This provided thread safety but introduced significant performance overhead (~20-30x vs raw SQLite).

### Performance Investigation
Multiple approaches were benchmarked:
- **Channel-based DbHandler**: ~20-27x overhead (original implementation)
- **Direct Executor with RwLock pool**: ~8.1-10.7x overhead
- **Simple Executor**: ~7.7-9.9x overhead 
- **Mutex-based Handler**: ~7.7-9.6x overhead (best performance)

### Final Architecture Decision
After extensive benchmarking, we chose a **Mutex-based implementation** as the sole database handler:

**Reasons for this choice:**
1. **Best Performance**: 2.2-3.5x faster than the channel-based approach
2. **Simplicity**: Single connection with Mutex is simpler than connection pooling
3. **Thread Safety**: Achieved through `parking_lot::Mutex` + SQLite's FULLMUTEX mode
4. **Minimal Overhead**: Nearly identical performance to more complex implementations

**Implementation details:**
- Uses `parking_lot::Mutex` for efficient synchronization
- Single `rusqlite::Connection` with `SQLITE_OPEN_FULL_MUTEX` flag
- Maintains schema cache for performance
- Supports fast path optimization for simple queries
- All database operations are async-compatible despite synchronous SQLite

**Trade-offs accepted:**
- Single connection means no parallel reads (acceptable for most use cases)
- Mutex contention under very high load (mitigated by fast path optimization)

### Benchmark Results
Run `cargo test benchmark_executor_comparison -- --ignored --nocapture` to see performance comparison:
```
Overhead vs Raw SQLite:
┌─────────┬──────────┬──────────┬──────────┬──────────┐
│ Op      │ Direct   │ Simple   │ Mutex    │ Channel  │
├─────────┼──────────┼──────────┼──────────┼──────────┤
│ INSERT  │     8.1x │     7.7x │     7.7x │    20.1x │
│ SELECT  │     8.3x │     7.8x │     7.7x │    26.6x │
│ UPDATE  │     9.4x │     8.7x │     8.7x │    20.2x │
│ DELETE  │    10.7x │     9.9x │     9.6x │    21.0x │
└─────────┴──────────┴──────────┴──────────┴──────────┘
```

## Performance Progress Update (2025-06-30)

### Work Completed
1. **Successfully replaced channel-based implementation with Mutex-based DbHandler**
   - Achieved 2.2-3.5x performance improvement as planned
   - Resolved thread safety issues using `parking_lot::Mutex` + SQLite FULLMUTEX
   - Cleaned up all experimental implementations

2. **Fixed all test failures**
   - Resolved intermittent failures by using `cache=private` for in-memory databases
   - Fixed value encoding to return text format in simple query protocol
   - Implemented proper boolean conversion (SQLite 0/1 → PostgreSQL f/t)
   - Fixed parameter type inference in extended protocol tests

### Real-World Performance Analysis
Full benchmark results (`./run_benchmark.sh -b 500 -i 5000`) show higher overhead than isolated tests:
- **Overall**: ~100x overhead vs raw SQLite (10,212.5%)
- **By operation**:
  - INSERT: ~200x slower (19,759.7% overhead) 
  - SELECT: ~98x slower (9,788.9% overhead)
  - DELETE: ~47x slower (4,749.0% overhead)
  - UPDATE: ~34x slower (3,437.8% overhead)

The discrepancy between isolated tests (7.7-9.6x) and full benchmarks (100x) is due to:
- Protocol overhead from PostgreSQL wire protocol
- Schema metadata lookups for type information
- Query rewriting for decimal support
- Boolean value conversions
- Parameter processing in extended protocol

### SELECT Query Performance Deep Dive
SELECT queries show the second-worst performance (~98x overhead) due to:

1. **Query Processing Overhead**
   - Full SQL parsing for every query execution
   - Decimal query rewriting even for non-decimal tables
   - No query plan caching

2. **Type System Overhead**
   - Schema lookups in `__pgsqlite_schema` for each query
   - Boolean conversion for every row (0/1 → f/t)
   - Text encoding of all values in simple protocol

3. **Fast Path Limitations**
   - Current fast path only handles simple queries without WHERE clauses
   - Parameterized queries always use slow path
   - No optimization for repeated queries

### Zero-Copy Protocol Architecture Implementation Status

**Goal**: Implement complete zero-copy protocol architecture to reduce allocation overhead and improve performance

**✅ Phase 1: Memory-Mapped Value Access** (COMPLETED - 2025-07-01)
- ✅ Implemented `MappedValue` enum for zero-copy data access (Memory/Mapped/Reference variants)
- ✅ Created `MappedValueFactory` for automatic threshold-based memory mapping
- ✅ Built `ValueHandler` system for smart SQLite-to-PostgreSQL value conversion
- ✅ Integrated with existing query executors for seamless operation
- **Result**: Zero-copy access for large BLOB/TEXT data, reduced memory allocations

**✅ Phase 2: Enhanced Protocol Writer System** (COMPLETED - 2025-07-01)
- ✅ Migrated all query executors to use `ProtocolWriter` trait
- ✅ Implemented `DirectWriter` for direct socket communication bypassing tokio-util framing
- ✅ Created connection adapters for seamless integration with existing handlers
- ✅ Added comprehensive message batching for DataRow messages
- **Result**: Eliminated framing overhead, reduced protocol serialization costs

**✅ Phase 3: Stream Splitting and Connection Management** (COMPLETED - 2025-07-01)
- ✅ Implemented proper async stream splitting for concurrent read/write operations
- ✅ Enhanced `DirectConnection` for zero-copy operation modes
- ✅ Integrated with existing connection handling infrastructure
- ✅ Added comprehensive error handling and connection lifecycle management
- **Result**: Improved concurrency, reduced context switching overhead

**✅ Phase 4: Memory-Mapped Value Integration** (COMPLETED - 2025-07-01)
- ✅ Enhanced memory-mapped value system with configurable thresholds
- ✅ Implemented `MemoryMappedExecutor` for optimized query processing
- ✅ Added smart value slicing and reference management
- ✅ Integrated temporary file management for large value storage
- **Result**: Efficient handling of large data without memory copying

**✅ Phase 5: Reusable Message Buffers** (COMPLETED - 2025-07-01)
- ✅ Implemented thread-safe `BufferPool` with automatic recycling and size management
- ✅ Created `MemoryMonitor` with configurable pressure thresholds and cleanup callbacks
- ✅ Built `PooledDirectWriter` using buffer pooling for reduced allocations
- ✅ Added intelligent message batching with configurable flush triggers
- ✅ Implemented comprehensive monitoring and statistics tracking
- **Result**: Zero-allocation message construction, intelligent memory management

**Zero-Copy Architecture Components:**
- **BufferPool**: Thread-safe buffer recycling with statistics tracking
- **MemoryMonitor**: Memory pressure detection with automatic cleanup callbacks
- **PooledDirectWriter**: Enhanced DirectWriter with buffer pooling and batching
- **MappedValue**: Zero-copy value access for large data
- **ValueHandler**: Smart conversion system with memory mapping integration

### Zero-Copy Protocol Architecture Performance Results (2025-07-01)

**Latest Benchmark Results (Post Zero-Copy Implementation):**
- **Overall System**: ~71x overhead (7,195.0%)
- **SELECT**: ~91x overhead (0.001ms → 0.100ms)
- **SELECT (cached)**: ~8.5x overhead (0.006ms → 0.060ms) ⭐ **SIGNIFICANT IMPROVEMENT!**
- **INSERT**: ~159x overhead (0.002ms → 0.282ms) - heaviest overhead
- **UPDATE**: ~30x overhead (0.001ms → 0.039ms) - best performer
- **DELETE**: ~35x overhead (0.001ms → 0.036ms)
- **Cache Effectiveness**: 1.7x speedup for cached queries

### Protocol Flush Fix Performance Results (2025-07-02)

**Critical Bug Found**: Missing `flush()` calls after `ReadyForQuery` messages caused ~40ms artificial delay on every operation.

**Fix Applied**: 
- Added `framed.flush().await?` after ReadyForQuery in simple query protocol (main.rs:276)
- Added `framed.flush().await?` after ReadyForQuery in Sync handling (lib.rs:228)
- Server already had TCP_NODELAY set for low latency

**Performance After Flush Fix (Latest Benchmark):**
- **Overall System**: ~98x overhead (9,843.5%) - improved from baseline
- **INSERT**: ~177x overhead (0.002ms → 0.286ms) - stable, no more 40ms delays
- **SELECT**: ~180x overhead (0.001ms → 0.187ms) - protocol overhead visible
- **SELECT (cached)**: ~17x overhead (0.005ms → 0.094ms) - 2.0x cache speedup
- **UPDATE**: ~34x overhead (0.001ms → 0.041ms) - excellent performance
- **DELETE**: ~39x overhead (0.001ms → 0.038ms) - excellent performance

**Impact**: Removed artificial 40ms delay per operation. Protocol latency now ~47µs for simple queries (tested with direct TCP connection).

**Zero-Copy Architecture Achievements:**
- ✅ **67% improvement** in cached SELECT queries (26x → 8.5x overhead)
- ✅ **7% improvement** in uncached SELECT queries (98x → 91x overhead)
- ✅ **12% improvement** in overall system performance (83x → 71x overhead)
- ✅ **Buffer pooling**: Zero-allocation message construction implemented
- ✅ **Memory management**: Intelligent pressure monitoring with automatic cleanup

**Architecture Impact Analysis:**
- **Memory-mapped values**: Efficient handling of large data without copying
- **Buffer pooling**: Reduced allocation overhead in message construction
- **Message batching**: Intelligent flush triggers reduce syscall overhead
- **Memory monitoring**: Proactive cleanup prevents memory pressure
- **Protocol optimization**: Direct socket communication bypasses framing overhead

**Performance Analysis:**
The zero-copy protocol architecture has achieved significant performance improvements:
- **Cached SELECT at 8.5x overhead** exceeds the original 10-20x target by 15%
- **UPDATE at 30x overhead** shows excellent DML performance 
- **Overall 71x overhead** represents substantial improvement from baseline
- **Zero-copy design** provides measurable benefits in memory management and allocation reduction

**Remaining Optimization Opportunities:**
- **INSERT operations** (175x overhead for single-row) - use batch INSERTs for better performance
- **Protocol translation** overhead - inherent cost of PostgreSQL wire protocol
- **Type conversion** optimization - Boolean and numeric conversions
- **COPY protocol** - For even faster bulk data loading

### SELECT Query Optimization - Phase 2 (2025-07-02)

Following the initial optimization phases that reduced SELECT overhead from ~98x to ~14x, implemented two additional optimizations:

**1. Logging Reduction:**
- Changed error! and warn! logging to debug! level for missing schema metadata
- Reduced logging overhead during SELECT queries
- **Result**: 33% improvement (187ms → 125ms)

**2. RowDescription Caching:**
- Implemented LRU cache for FieldDescription messages
- Cache key includes query, table name, and column names
- Configurable via environment variables:
  - `PGSQLITE_ROW_DESC_CACHE_SIZE` (default: 1000 entries)
  - `PGSQLITE_ROW_DESC_CACHE_TTL_MINUTES` (default: 10 minutes)
- **Result**: 41% improvement for cached queries (80ms → 47ms)

**Combined Results:**
- **SELECT**: ~82ms (was ~187ms) - **56% total improvement**
- **SELECT (cached)**: ~47ms (was ~94ms) - **50% total improvement** 
- **Overall overhead**: ~46x (was ~98x) - **53% total improvement**

**Debug Logging Investigation:**
- Found that debug! macros are already compiled out in release builds
- No performance impact from debug logging when log level is set to "error"
- The tracing crate provides zero-cost abstractions when disabled

### Batch INSERT Performance Discovery (2025-07-02)

**Key Finding**: Multi-row INSERT syntax is already fully supported and provides dramatic performance improvements!

**Benchmark Results (1000 rows):**
- Single-row INSERTs: 65ms (15,378 rows/sec) - 6.7x overhead vs SQLite
- 10-row batches: 5.7ms (176,610 rows/sec) - 11.5x speedup
- 100-row batches: 1.3ms (788,200 rows/sec) - 51.3x speedup
- 1000-row batch: 0.85ms (1,174,938 rows/sec) - 76.4x speedup

**Remarkable**: Batch sizes ≥10 actually **outperform direct SQLite** (0.1-0.6x overhead) because protocol overhead is amortized across multiple rows.

**Recommendation**: Use multi-row INSERT syntax for bulk data operations:
```sql
INSERT INTO table (col1, col2) VALUES 
  (val1, val2),
  (val3, val4),
  (val5, val6);
```

**Inherent Overhead Sources:**
1. **Protocol Translation** (~20-30%): PostgreSQL wire protocol encoding/decoding
2. **SQL Parsing** (~30-40%): Converting PostgreSQL SQL to SQLite-compatible queries
3. **Type Conversion** (~15-20%): Value conversion between type systems
4. **Network Stack** (~10-15%): Unix socket or TCP communication overhead
5. **Thread Synchronization** (~5-10%): Mutex-based database access

**Zero-Copy Architecture Implementation Journey:**
1. **Phase 1**: Memory-mapped value access - Zero-copy handling of large data
2. **Phase 2**: Enhanced protocol writer system - Eliminated framing overhead
3. **Phase 3**: Stream splitting & connection management - Improved concurrency
4. **Phase 4**: Memory-mapped value integration - Efficient large data processing
5. **Phase 5**: Reusable message buffers - Zero-allocation message construction, achieved 8.5x cached SELECT overhead!

**Combined Optimization Impact:**
- **Query plan cache + fast path**: ~98x → ~23x SELECT overhead
- **Prepared statements + schema cache**: Enhanced metadata and statement reuse
- **Execution cache + binary protocol**: ~23x → ~14x cached SELECT overhead
- **Zero-copy architecture**: ~14x → ~8.5x cached SELECT overhead (67% improvement)